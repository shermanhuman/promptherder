# Decisions: prompt-refactor

| #   | Idea                                                                                                         | Verdict  | Pros                                                                                                   | Cons                                  | Rationale                                                                                                                      |
| --- | ------------------------------------------------------------------------------------------------------------ | -------- | ------------------------------------------------------------------------------------------------------ | ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ |
| 1   | Add visual anchors to plan presentation (filesystem trees, interface sketches, happy path flow)              | accepted | Gives user spatial understanding of what's being built before approving                                | Adds length to plan output            | User explicitly wants this; the extra context prevents blind approval                                                          |
| 2   | Add TDD-first checkpoint in plan steps (tests before code)                                                   | accepted | Enforces discipline we already claim to follow                                                         | Already in TDD skill                  | Plan template should explicitly sequence test→code→verify per step                                                             |
| 3   | Add smoke test section to execute finish                                                                     | accepted | User asked for manual validation guidance                                                              | Slightly more work per execute        | Walk-through is cheap and catches integration issues automated tests miss                                                      |
| 4   | Overhaul review checklist with concrete criteria per domain                                                  | accepted | Current checklist is vague; user struggled to find findings                                            | Longer skill file                     | Precision beats brevity for a reviewer role                                                                                    |
| 5   | Add YAGNI/DRY/overengineering as explicit review domains                                                     | accepted | Missing from current checklist entirely                                                                | Overlaps slightly with existing items | These are distinct enough to warrant own sections                                                                              |
| 6   | Run review domains in parallel                                                                               | accepted | Each domain is independent; parallelism speeds review                                                  | None                                  | Already stated as a pattern in parallel skill                                                                                  |
| 7   | Add senior engineer persona to review skill                                                                  | accepted | Frames the review mindset correctly                                                                    | Persona instructions can be ignored   | Low cost, high value for framing                                                                                               |
| 8   | Add structured output formatting rules (headers, dividers, emoji, tables)                                    | accepted | User found output hard to read; web research confirms structured markdown is best practice             | Could become overengineered           | Keep rules concise: use semantic emoji sparingly, enforce header hierarchy, use tables for findings                            |
| 9   | Add finding IDs (B1, M2, etc.) as mandatory in review output                                                 | accepted | User couldn't find m2 when referenced                                                                  | Minor overhead                        | Critical for cross-referencing between review and fix discussions                                                              |
| 10  | Enforce parallel research in ALL skills that have a research phase                                           | accepted | TDD, debug, review, plan all have research phases but only some say "parallel"                         | None                                  | Consistency                                                                                                                    |
| 11  | Add plan reference links in plan template (link to plan.md, decisions.md)                                    | accepted | User didn't know these existed during execution                                                        | None                                  | Zero cost                                                                                                                      |
| 12  | Create separate formatting skill or embed in compound-v.md                                                   | rejected | Clean separation                                                                                       | Overkill for a formatting guide       | Embed formatting rules in compound-v.md (the always-on rule file). All workflows inherit it. YAGNI.                            |
| 13  | Edge cases & error handling as its own review domain (not folded into correctness)                           | accepted | Boundary conditions, resource cleanup, error propagation are distinct from "does it meet requirements" | Slightly more domains                 | Research confirms edge cases need dedicated focus: nil/empty/max, race conditions, resource leaks, error wrapping with context |
| 14  | Logging & observability as a review domain                                                                   | accepted | Missing entirely; structured logging, log levels, sensitive data in logs are common production issues  | Adds another domain                   | Low cost, high value — catches secrets-in-logs, missing context, wrong log levels                                              |
| 15  | Correctness domain references plan.md and future-tasks.md                                                    | accepted | Grounds the review in the original requirements; prevents drift                                        | Requires plan to exist                | Already a precondition of /execute                                                                                             |
| 16  | Review research phase downloads version-specific docs from stack.md                                          | accepted | Catches deprecated APIs, version-specific gotchas, breaking changes                                    | Slightly slower                       | Web research already happens; scoping it to pinned versions is strictly better                                                 |
| 17  | Per-domain stack research: edge cases, security, performance each search for version-specific issues         | accepted | Each domain has different search queries (gotchas vs CVEs vs anti-patterns)                            | 3 extra web searches per review       | Searches are parallel and free; the version-specific findings are high-signal                                                  |
| 18  | Findings-first workflow: present findings, wait for user, then fix                                           | accepted | User visibility before changes; user might disagree with a finding                                     | Extra round-trip                      | Superpowers pattern confirmed: present → triage → fix. Prevents surprise changes.                                              |
| 19  | Strengths section in review output                                                                           | accepted | Context + morale; acknowledges what's well done                                                        | Slightly longer output                | Specific file:line strengths are high-signal context, not fluff                                                                |
| 20  | File:line references mandatory for every finding                                                             | accepted | "Be specific, not vague" — superpowers lesson                                                          | More work per finding                 | Essential for cross-referencing; finding without location is useless                                                           |
| 21  | "How to fix" per finding                                                                                     | accepted | Actionable findings > vague observations                                                               | Takes more tokens                     | Only "if not obvious" — skip for trivial fixes                                                                                 |
| 22  | Verdict: "Ready to merge? Yes/No/With fixes"                                                                 | accepted | Clear decision > vague "summary + next actions"                                                        | None                                  | Concrete verdict forces honest assessment                                                                                      |
| 23  | Git diff review: use `git diff BASE..HEAD` for review scope                                                  | accepted | More rigorous than reading files; catches removals, renames, context around changes                    | Requires git history                  | Already using git; this is strictly better                                                                                     |
| 24  | YAGNI grep technique: search for actual callers before accepting abstractions                                | accepted | Concrete technique for Domain 8 checks                                                                 | None                                  | "If nothing calls it, flag it" — simple, effective                                                                             |
| 25  | Idiomatic code check in Domain 6 with version-specific research                                              | accepted | Code should use natural language idioms (constructs, style guide, stdlib over reinventing)             | Another search per review             | Idiomatic code reduces cognitive overhead; research confirms it's a core review dimension                                      |
| 26  | Domain targeting: `/review correctness` runs only that domain                                                | accepted | Focused reviews for specific concerns; faster feedback loop                                            | Adds routing logic to workflow        | Simple pattern match; each domain is already independent                                                                       |
| 27  | YOLO mode cascades: /review YOLO auto-fixes, /execute YOLO auto-reviews+fixes, /plan YOLO runs full pipeline | accepted | Full autonomous operation for trusted changes; zero interaction                                        | Risk of unwanted changes              | User explicitly opts in with all-caps flag; summary always output; each stage still does real work (research, tests, review)   |
